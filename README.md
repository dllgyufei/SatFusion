# ğŸŒ SatFusion / SatFusion*: A Unified Framework for Enhancing Remote Sensing Images via Multi-Frame and Multi-Source Fusion

> Official Implementation of **SatFusion** and its enhanced variant **SatFusion***  
> **SatFusion: A Unified Framework for Enhancing Remote Sensing Images via Multi-Frame and Multi-Source Image Fusion**
---

## ğŸ›°ï¸ Overview
**SatFusion** is a unified deep learning framework for enhancing remote sensing imagery by jointly fusing **Multi-Frame** and **Multi-Source** data.

Unlike existing approaches that treat **Multi-Frame Super-Resolution (MFSR)** and **Pansharpening** as separate tasks, SatFusion integrates both into a single end-to-end architecture, enabling improved **reconstruction quality, robustness, and generalization**.

**SatFusion*** is an enhanced variant of SatFusion that further improves robustness under **temporal misalignment and real-world degradations** by incorporating **PAN-guided multi-frame feature encoding**.

![](SatFuion.png)

---

## ğŸ”¬ Key Features
- **Unified Fusion Framework** â€” Joint multi-frame and multi-source optimization within a single architecture  
- **Multi-Temporal Image Fusion (MTIF)** â€” Aggregates complementary information from multiple LRMS frames  
- **Multi-Source Image Fusion (MSIF)** â€” Injects fine-grained spatial details from high-resolution PAN images  
- **Fusion Composition Module** â€” Adaptive feature fusion with residual connections and spectral refinement  
- **SatFusion*** â€” PAN-guided multi-frame encoding for enhanced robustness to misalignment and noise
---

## ğŸ§© Architecture
SatFusion consists of three core modules.  
SatFusion* follows the same overall architecture, with an enhanced PAN-guided design in the multi-temporal fusion stage.

### 1. Multi-Temporal Image Fusion (MTIF)
- **Inputs:** multiple LRMS images  
- **Outputs:** temporally enhanced MS features  
- **Implementation:** shared encoders, feature aggregation, and PixelShuffle-based decoding  
- **SatFusion\*:** incorporates PAN-guided feature encoding to improve temporal alignment and stability
### 2. Multi-Source Image Fusion (MSIF)
- **Inputs:** MTIF output + high-resolution PAN image  
- **Outputs:** spatially enhanced MS feature map  
- **Implementation:** PAN-guided spatial detail injection following pansharpening principles
### 3. Fusion Composition Module
- Combines multi-temporal and multi-source features  
- Employs residual connections and 1Ã—1 convolutions for adaptive spectral balancing
---

## ğŸŒ Dataset
We conduct experiments on the WorldStrat, WV3, QB, and GF2 datasets.
The complete WorldStrat dataset can be downloaded from [https://worldstrat.github.io/](https://worldstrat.github.io/), while the WV3, QB, and GF2 datasets are available at [https://liangjiandeng.github.io/PanCollection.html](https://liangjiandeng.github.io/PanCollection.html).

This repository provides the **complete experimental pipeline** for the **WorldStrat** dataset.  
For the **WV3, QB, and GF2** datasets, we provide the **core implementation**, including the model architecture and training configurations, which are implemented based on the **DLPan-Toolbox** framework  
([https://github.com/liangjiandeng/DLPan-Toolbox.git](https://github.com/liangjiandeng/DLPan-Toolbox.git)).


## âš™ï¸ Quick Start

### ğŸ›°ï¸ WorldStrat Dataset
This section describes the complete training, validation, and testing pipeline for the **WorldStrat** dataset.

#### ğŸ“ Code Architecture for the Worldstrat Dataset
```
code_worldstrat/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Train_Val_Test.py
â”œâ”€â”€ train.py
â”œâ”€â”€ Inference.py
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ datasets.py
    â”œâ”€â”€ datasources.py
    â”œâ”€â”€ lightning_modules.py
    â”œâ”€â”€ loss.py
    â”œâ”€â”€ modules.py
    â”œâ”€â”€ plot.py
    â”œâ”€â”€ train.py
    â”œâ”€â”€ transforms.py
    â”œâ”€â”€ misr/
    â”‚   â”œâ”€â”€ misr_public_modules.py
    â”‚   â”œâ”€â”€ gener_new.py
    â”‚   â”œâ”€â”€ highresnet.py
    â”‚   â”œâ”€â”€ rams.py
    â”‚   â”œâ”€â”€ srcnn.py
    â”‚   â””â”€â”€ trnet.py
    â””â”€â”€ sharp/
        â”œâ”€â”€ pnn/
        |   â”œâ”€â”€ DoubleConv2d.py
        |   â””â”€â”€ pnn.py
        â”œâ”€â”€ pannet/
        |   â””â”€â”€ pannet.py
        â”œâ”€â”€ u2net/
        |   â”œâ”€â”€ s2block.py
        |   â””â”€â”€ u2net.py
        â”œâ”€â”€ mamba/
        |   â”œâ”€â”€ mamba_module.py
        |   â”œâ”€â”€ panmamba_baseline_finalversion.py
        |   â””â”€â”€ refine_mamba.py
        â”œâ”€â”€ ARConv/
        |   â”œâ”€â”€ ARConv.py
        |   â””â”€â”€ model.py       
        â””â”€â”€ psit/ (not used)
            â”œâ”€â”€ GPPNN_PSIT.py
            â”œâ”€â”€ modules_psit.py
            â””â”€â”€ refine_psit.py
```
> The folders `src/misr` and `src/sharp` contain sub-modules for MFSR and Pan-Sharpening, respectively.  
> The file `src/modules.py` serves as the backbone of **SatFusion**.

#### ğŸ§± Environment Setup

- **CUDA:** 11.8+  
- **Python:** 3.10+  
- **PyTorch:** 2.4.0+  

Install additional dependencies:
```bash
pip install -r requirements.txt
```
To run the block of Pan-Mamba , Vision-Mamba is required.You can refer to the guidance in [Pan-Mamba](https://github.com/alexhe101/pan-mamba) and this blog [Install Vision Mamba on Linux](https://zhuanlan.zhihu.com/p/687359086).

#### ğŸš€ Training, Validation & Testing

Set the params `root` as your root dir of the dataset and `list_of_aios` as `"pretrained_model/final_split.csv"` in file `Train_Val_Test.py`. Run `Train_Val_Test.py` to train, validate, and test the model.

The process of training is visible on [Weights & Biases](https://wandb.ai). Replace the `project` and `entity` in `src/train.py`. For details, refer to [Weights & Biases quickstart guide](https://wandb.ai/quickstart?).


### ğŸ›°ï¸ WV3 / QB / GF2 Datasets (Core Implementation)

For the WV3, QB, and GF2 datasets, we provide the core implementation of **SatFusion / SatFusion***,
including the model architecture and training configurations.
The experiments are implemented based on the **DLPan-Toolbox** framework.
Please refer to the official repository ([https://github.com/liangjiandeng/DLPan-Toolbox.git](https://github.com/liangjiandeng/DLPan-Toolbox.git)) for dataset preparation and execution details.

#### ğŸ“ Code Architecture
```
code_wv3_qb_gf2/
â””â”€â”€ pansharpening/
    â”œâ”€â”€ configs/
    â”‚   â”œâ”€â”€ hook_configs.py
    â”‚   â”œâ”€â”€ option_.py
    â”‚   â”œâ”€â”€ option_pnn.py
    â”‚   â”œâ”€â”€ option_trnet_pnn.py 
    â”‚   â”œâ”€â”€ option_trnet_pan__pnn.py 
    â”‚   â””â”€â”€ ...
    â””â”€â”€ model/
        â”œâ”€â”€ PNN/
        â”‚   â”œâ”€â”€ model_pnn.py
        â”‚   â””â”€â”€ pnn_main.py
        â”œâ”€â”€ TRNet_PNN/
        â”‚   â”œâ”€â”€ model_trnet_pnn.py
        â”‚   â””â”€â”€ trnet_pnn_main.py
        â”œâ”€â”€ TRNet_PAN_PNN/
        â”‚   â”œâ”€â”€ model_trnet_pan_pnn.py
        â”‚   â””â”€â”€ trnet_pan_pnn_main.py
        â””â”€â”€ ...
```
#### ğŸ§© Directory Description

- **configs/**  
  Contains training configuration files for different methods and experimental
  settings, including: number of epochs, learning rate and optimizer...
  Each `option_*.py` file corresponds to a specific model configuration
  used in the experiments.

- **model/**  
  Contains the core implementation of each method.
  For each architecture (e.g., PNN, TRNet_PNN, TRNet_PAN_PNN,), the implementation
  is typically divided into two parts:
  - `*_main.py`: loss functions  
  - `model_*.py`: defines the network architecture  


---
## ğŸ¤ Issues and Contributions
If you encounter any issues or have suggestions for improvement, please feel free to open an issue in the GitHub issue tracker.   
We appreciate your use of SatFusion for your satellite image enhancement needs!We hope it proves to be a valuable framework.
