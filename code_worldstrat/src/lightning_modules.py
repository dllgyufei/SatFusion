from numpy import sqrt
import torch
from torch import nn, Tensor
import torch.nn.functional as F
from typing import Tuple, Optional
import pytorch_lightning as pl
import wandb
from argparse import ArgumentParser
from kornia.contrib import extract_tensor_patches
from torch.optim import Adam, SGD
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from torchmetrics import metric
from torchvision.utils import make_grid
from src.datasources import S2_ALL_12BANDS
from src.losses import mse_loss, mae_loss, ssim_loss, Shift, ergas, sam_loss, _ssim, _sam
import pandas as pd
import os
from src.modules import MultiTemporalGenerator
from src.misr.gener_new import MultiTemporalGenerator_new


def detach_dict(dict):
    """Detaches the tensors in a dictionary.

    Parameters
    ----------
    dict : dict of Tensor
        Dictionary containing the tensors to detach.

    Returns
    -------
    dict of Tensor
        Dictionary containing the detached tensors.
    """
    for tensor_key in dict:
        if dict[tensor_key].requires_grad:
            dict[tensor_key] = dict[tensor_key].detach()
    return dict


class LitModel(pl.LightningModule):
    """Lightning Module wrapper.
    Implements the forward, loss, train/val/test step functions and logging.
    Parses and adds default model arguments.
    """

    def __init__(self, backbone, **kws) -> None:
        """Initialises the LightningModule with the backbone and the default arguments.
        The hyperparameters are fetchd from the hparams attribute.

        Parameters
        ----------
        backbone : nn.Module
            Backbone model, as generated by train.py.
        """
        super().__init__()
        self.backbone = backbone
        self.save_hyperparameters(kws)
        self.use_artificial_dataset = self.hparams.use_artificial_dataset
        self.target_size = self.hparams.chip_size
        self.revisits = self.hparams.revisits
        self.temporal_noise = self.hparams.temporal_noise
        self.temporal_jitter = self.hparams.temporal_jitter
        self.transform = self.hparams.transform  # transform as a torch module

        output_channels = [channel for channel in range(self.backbone.out_channels)]

        self.gener_new = MultiTemporalGenerator_new(
            n_frames=self.revisits,
            target_size=self.target_size,
            # scale = 3,
            # kernel_size=5,
            # temporal_noise=self.temporal_noise,  # 3%噪声
            # temporal_jitter=self.temporal_jitter  # ±15%亮度变化
        )
        self.generator = MultiTemporalGenerator(
            n_frames=self.revisits,
            target_size=self.target_size,
            kernel_size=5,
            temporal_noise=self.temporal_noise,  # 3%噪声
            temporal_jitter=self.temporal_jitter  # ±15%亮度变化
        )

        self.check_loss_functions_weights_are_valid()
        self.create_shifters()
        self.create_metrics()
        self.enable_benchmark_logging()

    def enable_benchmark_logging(self):
        if self.hparams.benchmark:
            self.logging = {}
            self.logging['current_idx'] = 1
            self.logging['log'] = pd.DataFrame()
            self.log_path = './log'

    def check_loss_functions_weights_are_valid(self):
        """Checks that the loss functions weights all sum to 1."""
        sum_weights = self.hparams.w_mse + self.hparams.w_mae + self.hparams.w_ssim + self.hparams.w_sam
        assert abs(sum_weights - 1.0) < 1e-6, f"Loss weights must sum to 1.0, but got {sum_weights}"

    def create_shifters(self):
        """Creates a shifter for the training and validation steps."""

        self.shifter = Shift(
            shift_by_px=self.hparams.shift_px,
            mode=self.hparams.shift_mode,
            step=self.hparams.shift_step,
            use_cache=False,
        )

    def on_save_checkpoint(self, checkpoint):
        # 手动保存 backbone 的 state_dict
        checkpoint.update({
            "backbone_state_dict": self.backbone.state_dict(),
        }
        )

    def on_load_checkpoint(self, checkpoint):
        # 手动加载 backbone 的 state_dict
        self.backbone.load_state_dict(checkpoint["backbone_state_dict"])

    def create_metrics(self):
        """Creates metrics that use shifters when computing the losses (MSE, MAE, SSIM) for the train, validation and test sets."""
        self.metrics_hr = {}
        # TODO: dict of train/val/test metrics
        self.train_metrics = self.metrics_factory("train", self.shifter)
        self.val_metrics = self.metrics_factory("val", self.shifter)
        self.test_metrics = self.metrics_factory("test", self.shifter)

    def metrics_factory(self, prefix, shifter):
        """Creates a dictionary of metrics that use the shifter when computing the losses (MSE, MAE, SSIM) for the train, validation and test sets.

        Parameters
        ----------
        prefix : str
            Prefix for the metrics (train/val/test).
        shifter : losses.Shift
            Shifter to use when computing the losses.

        Returns
        -------
        dict
            Dictionary of metrics.
        """
        return {
            f"{prefix}/MSE": lambda y_hat, y: shifter.registered_loss(mse_loss)(y_hat, y),
            f"{prefix}/MAE": lambda y_hat, y: shifter.registered_loss(mae_loss)(y_hat, y),
            f"{prefix}/SSIM_loss": lambda y_hat, y: shifter.registered_loss(ssim_loss)(y_hat, y, window_size=5),
            f"{prefix}/ERGAS": lambda y_hat, y: shifter.registered_loss(ergas)(y_hat, y),
            f"{prefix}/SAM_loss": lambda y_hat, y: sam_loss(y_hat, y),  # nips
            f"{prefix}/SSIM": lambda y_hat, y: shifter.registered_loss(_ssim)(y_hat, y),
            f"{prefix}/SAM": lambda y_hat, y: shifter.registered_loss(_sam)(y_hat, y),
            # f"{prefix}/SAM_loss": lambda y_hat, y:shifter.registered_loss(sam_loss)(y_hat, y),  #
        }

    def forward(self, x: Tensor, pan: Optional[Tensor] = None, y: Optional[Tensor] = None):
        """Forward pass of the model.

        Parameters
        ----------
        x : Tensor
            Input tensor (LR) of dimensions (batch_size, channels, height, width).
        y : Tensor, optional
            Target tensor (HR) of dimensions (batch_size, channels, height, width).

        Returns
        -------
        Tensor
            Output tensor (y_hat) of dimensions (batch_size, channels, height, width).
        """
        return self.backbone(x=x, pan=pan, y=y)

    def save_logs(self):
        if hasattr(self, 'logging') and 'log' in self.logging:
            # 确保日志目录存在
            os.makedirs(self.log_path, exist_ok=True)

            # 保存日志文件
            log_file = os.path.join(self.log_path, "validation_log.csv")
            self.logging["log"].to_csv(log_file, index=False)
            print(f"Logs saved to {log_file}")

    def validation_log(self, y, m, prefix):

        log = pd.DataFrame(
            columns=[
                "MSE",
                "MAE",
                "SSIM_loss",
                "PSNR",
            ]
        )
        mse, mae, ssimloss, psnr = (
            m[f"{prefix}/MSE"],
            m[f"{prefix}/MAE"],
            m[f"{prefix}/SSIM_loss"],
            m[f"{prefix}/PSNR"],
        )
        for batch_item in range(y.shape[0]):  # batch_size
            log.loc[self.logging["current_idx"]] = (
                mse[batch_item].detach().item(),
                mae[batch_item].detach().item(),
                ssimloss[batch_item].detach().item(),
                psnr[batch_item].detach().item(),
            )
            self.logging["current_idx"] += 1
        self.logging["log"] = pd.concat([self.logging["log"], log])

    def loss(self, batch, metrics, prefix):
        """Computes the loss for a batch using the provided metrics.

        Parameters
        ----------
        batch : dict
            Dictionary with the batches for LR/HR/etc.
        metrics : dict of Callable
            Dictionary with the metrics to use.
        prefix : str
            Prefix for the metrics (train/val/test).

        Returns
        -------
        dict
            Dictionary with the losses.
        """

        batch = self.transform(batch)  # GPU/Batched data augmentation
        if self.use_artificial_dataset:
            #  使用人造数据集
            x_real, y_real, pan_real = batch["lr"], batch["hr"], batch["hr_pan"]

            x = self.gener_new.generate(y_real)  # DL_PAN
            # x = self.generator.generate(y_real)  #NIPS

            y = y_real
            pan = pan_real
        else:
            x, y, pan = batch["lr"], batch["hr"], batch["hr_pan"]

        y_hat, misr_out, sharpening_out = self(x=x, pan=pan, y=y)

        y, y_hat = y[:, 0], y_hat[:, 0]

        y_hat = self.bias_adjust(y, y_hat)

        self.initialise_shifters()

        m_y_hat, mse_y_hat, mae_y_hat, ssimloss_y_hat, samloss_y_hat = self.compute_metrics(y, y_hat, metrics, prefix,
                                                                                            pan)
        if misr_out != None and sharpening_out != None:  # MISR + Sharpening
            #整体损失
            loss = (
                    (self.hparams.w_mse * mse_y_hat)
                    + (self.hparams.w_mae * mae_y_hat)
                    + (self.hparams.w_ssim * ssimloss_y_hat)
                    + (self.hparams.w_sam * samloss_y_hat)
            )
        else:  # MISR + None 或者 None + Sharpening
            loss = (
                    (self.hparams.w_mse * mse_y_hat)
                    + (self.hparams.w_mae * mae_y_hat)
                    + (self.hparams.w_ssim * ssimloss_y_hat)
                    + (self.hparams.w_sam * samloss_y_hat)
            )

        if self.hparams.benchmark:
            self.validation_log(y, detach_dict(m_y_hat), prefix)

        return {
            "y_hat": y_hat,
            "y": y,
            "metrics": m_y_hat,
            "loss": loss,
        }

    @staticmethod
    def bias_adjust(y, y_hat):
        """Applies the brightness correction to the output.

        Parameters
        ----------
        y : torch.Tensor
            Target tensor (HR) of dimensions (batch_size, channels, height, width).
        y_hat : torch.Tensor
            Output tensor (SR) of dimensions (batch_size, channels, height, width).

        Returns
        -------
        torch.Tensor
            Output tensor with the brightness correction applied.
        """
        bias = (y - y_hat).mean(dim=(-1, -2), keepdim=True)  # bias / zero-order
        return y_hat + bias

    def initialise_shifters(self):
        """Initialises the shifter for the HR/SR pairs."""
        self.shifter.y, self.shifter.y_hat = None, None

        # NOTE: whatever gets cached early on, might be suboptimal.
        # So it would be good to update the cache periodically.
        # Clearing the cache every 50 epochs.
        if self.current_epoch % 50 == 0:
            self.clear_shifter_cache()

    def compute_metrics(self, y, y_hat, metrics, prefix, pan):
        """Computes the metrics for a batch (MSE, MAE, SSIM, PSNR).

        Parameters
        ----------
        y : torch.Tensor
            Target tensor (HR) of dimensions (batch_size, channels, height, width).
        y_hat : torch.Tensor
            Output tensor (SR) of dimensions (batch_size, channels, height, width).
        metrics : dict of Callable
            Dictionary with the metric functions to use.
        prefix : str
            Prefix for the metrics (train/val/test).

        Returns
        -------
        dict, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor
            Dictionary with the metrics (MSE, MAE, SSIM, PSNR).
            The metrics themselves, the MSE, MAE, SSIM
        """
        metrics = {
            metrics_function: metrics[metrics_function](y_hat, y)
            for metrics_function in metrics
        }
        mse = metrics[f"{prefix}/MSE"].clone()
        mae = metrics[f"{prefix}/MAE"].clone()
        ssimloss = metrics[f"{prefix}/SSIM_loss"].clone()
        samloss = metrics[f"{prefix}/SAM_loss"].clone()
        metrics[f"{prefix}/PSNR"] = -10.0 * torch.log10(mse)
        return metrics, mse, mae, ssimloss, samloss

    def clear_shifter_cache(self):
        """Clears the cache of the output shifters."""

        self.shifter.shift_cache = {}

    def training_step(self, batch):
        """Training step.
        Calls the forward pass, computes the loss and metrics for the training batch.
        Logs the reduced metrics.

        Parameters
        ----------
        batch : dict
            Dictionary with the batches for LR/HR/etc.

        Returns
        -------
        dict
            Dictionary with the training loss and metrics.
        """
        loss_output = self.loss(
            batch, self.train_metrics, prefix="train"
        )

        metrics, metrics_reduced = self.unpack_and_reduce_metrics(
            loss_output, prefix="train"
        )

        self.log_dict(metrics_reduced, on_step=False, on_epoch=True)
        return {"loss": metrics_reduced["train/loss"], "metrics": metrics}

    def unpack_and_reduce_metrics(self, loss_output, prefix):
        """Unpacks the metrics for a split (prefix) and reduces them (mean).

        Parameters
        ----------
        loss_output : dict
            Dictionary with the loss and metrics.
        prefix : str
            Prefix for the metrics (train/val/test).

        Returns
        -------
        dict
            Dictionary with the reduced (mean) metrics.
        """
        metrics = loss_output["metrics"]
        metrics_reduced = {
            metric_name: metric.mean() for metric_name, metric in metrics.items()
        }
        metrics_reduced[f"{prefix}/loss"] = loss_output["loss"].mean()
        return metrics, metrics_reduced

    def validation_step(self, batch, batch_idx):
        """Validation step.
        Calls the forward pass, computes the loss and metrics for the validation batch.
        Logs the reduced metrics.

        Parameters
        ----------
        batch : dict
            Dictionary with the batches for LR/HR/etc.

        Returns
        -------
        torch.Tensor
            Validation loss.
        """
        loss_output = self.loss(
            batch, self.val_metrics, prefix="val"
        )
        _, metrics_reduced = self.unpack_and_reduce_metrics(loss_output, prefix="val")
        self.log_dict(metrics_reduced)
        return loss_output["loss"]  # Instance-wise loss

    def validation_epoch_end(self, validation_step_output):
        """Validation epoch end.
        Concatenates the validation losses and logs them as a histogram to WandB.

        Parameters
        ----------
        validation_step_output : dict
            Dictionary with the validation losses.
        """
        instance_loss = torch.cat(validation_step_output).to("cpu")
        try:
            self.logger.experiment.log(
                {
                    "val/loss_hist": wandb.Histogram(instance_loss),
                    "global_step": self.global_step,
                }
            )
        except ValueError as e:
            print(e)

        if self.hparams.benchmark:
            self.save_logs()

    def predict_step(self, batch, batch_idx):
        batch = self.transform(batch)  # GPU/Batched data augmentation
        if self.use_artificial_dataset:
            #  使用人造数据集
            x_real, y_real, pan_real = batch["lr"], batch["hr"], batch["hr_pan"]
            x = self.generator.generate(y_real)
            y = y_real
            pan = pan_real
        else:
            x, y, pan = batch["lr"], batch["hr"], batch["hr_pan"]
        y_hat, _, _ = self(x=x, pan=pan, y=None)

        y, y_hat = y[:, 0], y_hat[:, 0]
        y_hat = self.bias_adjust(y, y_hat)

        return y_hat

    def test_step(self, batch, batch_idx):
        """Test step.+
        Calls the forward pass, computes the loss and metrics for the test batch.
        Logs the reduced metrics.

        Parameters
        ----------
        batch : dict
            Dictionary with the batches for LR/HR/etc.

        Returns
        -------
        torch.Tensor
            Test loss.
        """
        loss_output = self.loss(
            batch, self.test_metrics, prefix="test"
        )

        _, metrics_reduced = self.unpack_and_reduce_metrics(loss_output, prefix="test")

        # 为 metrics_reduced 添加前缀
        metrics_reduced_prefixed = {f"model/{k}": v for k, v in metrics_reduced.items()}

        # 合并字典并记录到 WandB
        combined_metrics = {**metrics_reduced_prefixed}
        self.log_dict(combined_metrics)

        return loss_output["loss"]  # Instance-wise loss

    def configure_optimizers(self):
        """Configures the optimizers for the model based on the hyperparameters.
        Uses the learning rate and weight decay parameters for the Adam optimizer.
        Uses the learning rate, weight decay and momentum parameters for the SGD optimizer.

        Uses the cos_anneal_T_0 hyperparameter for the cosine annealing scheduler.
        See: https://arxiv.org/abs/1608.03983

        Returns
        -------
        dict
            Dictionary containing the optimizer, scheduler and monitor key (val/loss).
        """
        if self.hparams.optimizer == "adam":
            optimizer = Adam(
                self.parameters(),
                lr=self.hparams.learning_rate,
                weight_decay=self.hparams.weight_decay,
            )

        elif self.hparams.optimizer == "sgd":
            optimizer = SGD(
                self.parameters(),
                lr=self.hparams.learning_rate,
                weight_decay=self.hparams.weight_decay,
                momentum=self.hparams.momentum,
            )
        scheduler = CosineAnnealingWarmRestarts(
            optimizer, T_0=self.hparams.cos_anneal_T_0
        )
        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)
        return {
            "optimizer": optimizer,
            "lr_scheduler": scheduler,
            "monitor": "val/loss",
        }

    @staticmethod
    def add_model_specific_args(parent_parser):
        """Add model-specific arguments (and their default values) to the parser.

        Parameters
        ----------
        parent_parser : argparse.ArgumentParser
            argparse.ArgumentParser to add to.

        Returns
        -------
        ArgumentParser
            ArgumentParser with the model-specific arguments added.
        """
        parser = ArgumentParser(parents=[parent_parser], add_help=False)
        # Model arguments
        parser.add_argument("--model", default="srcnn", type=str)
        parser.add_argument("--ourMISRmodel", default="None", type=str)
        parser.add_argument("--ourSharpeningmodel", default="None", type=str)
        parser.add_argument("--hidden_channels", default=64, type=int)
        parser.add_argument("--homography_fc_size", default=128, type=int)
        parser.add_argument("--kernel_size", default=3, type=int)
        parser.add_argument("--zoom_factor", default=2, type=int)
        parser.add_argument("--sr_kernel_size", default=1, type=int)

        # Optimizer arguments
        parser.add_argument("--optimizer", default="adam", type=str)
        parser.add_argument("--learning_rate", default=5e-4, type=float)
        parser.add_argument("--learning_rate_decay", default=0.97, type=float)
        parser.add_argument("--learning_rate_patience", default=3, type=int)
        parser.add_argument("--momentum", type=float, default=0.9)
        parser.add_argument("--weight_decay", type=float, default=1e-4)
        parser.add_argument("--cos_anneal_T_0", type=int, default=300)

        # Data arguments
        parser.add_argument("--input_size", nargs="+", default=(400, 400), type=int)
        parser.add_argument("--output_size", nargs="+", default=(1000, 1000), type=int)
        parser.add_argument("--chip_size", nargs="+", type=int, default=(50, 50))
        parser.add_argument("--chip_stride", nargs="+", type=int, default=None)
        parser.add_argument("--revisits", default=8, type=int)
        parser.add_argument(
            "--use_reference_frame",
            action="store_true",
            help="Pair each revisit with a reference (median) frame.",
        )
        parser.add_argument(
            "--use_artificial_dataset",
            action="store_true",
            help="Whether to use the artificial dataset. Set to True when --use_artificial_dataset is provided."
        )
        parser.add_argument(
            "--use_sampling_model",
            action="store_true",
            help="Whether to use the sampling module. Set to True if --use_sampling is provided."
        )
        # 使用人造数据集时，可以适当加入噪声
        parser.add_argument("--temporal_noise", default=0.03, type=float)
        parser.add_argument("--temporal_jitter", default=0.1, type=float)

        # Shifter arguments
        parser.add_argument("--registration_kind", default=None, type=str)
        parser.add_argument("--shift_px", default=2, type=int)
        parser.add_argument("--shift_mode", default="lanczos", type=str)
        parser.add_argument("--shift_step", default=0.5, type=float)

        # Loss weights: MSE, MAE, SSIM
        parser.add_argument("--w_mse", default=1.0, type=float)
        parser.add_argument("--w_mae", default=0.0, type=float)
        parser.add_argument("--w_ssim", default=0.0, type=float)
        parser.add_argument("--w_sam", default=0.0, type=float)

        return parser


class ImagePredictionLogger(pl.Callback):
    """Logs the model losses, inputs and outputs to WandB.

    Parameters
    ----------
    pl : pl.LightningModule
        Lightning module containing the model.
    """

    def __init__(
            self,
            train_dataloader,
            val_dataloader,
            test_dataloader,
            revisits,
            temporal_noise,
            temporal_jitter,
            log_every_n_epochs=1,
            window_size=None,
            use_artificial_dataset=False,
    ):
        """Initialize the logger.

        Parameters
        ----------
        train_dataloader : torch.utils.data.DataLoader
            Training dataloader.
        val_dataloader : torch.utils.data.DataLoader
            Validation dataloader.
        test_dataloader : torch.utils.data.DataLoader
            Test dataloader.
        log_every_n_epochs : int, optional
            Log every n epochs.
        window_size : tuple of int, optional
            Size of the window to use for the patches.
        """
        super().__init__()
        self.train_dataloader = train_dataloader
        self.val_dataloader = val_dataloader
        self.test_dataloader = test_dataloader
        self.log_every_n_epochs = log_every_n_epochs
        self.window_size = window_size
        self.revisits = revisits
        self.use_artificial_dataset = use_artificial_dataset
        self.temporal_noise = temporal_noise
        self.temporal_jitter = temporal_jitter

        self.gener_new = MultiTemporalGenerator_new(
            n_frames=self.revisits,
            target_size=(50, 50),
        )

        self.generator = MultiTemporalGenerator(
            n_frames=self.revisits,
            target_size=(50, 50),
            kernel_size=5,
            temporal_noise=self.temporal_noise,  # 30%噪声
            temporal_jitter=self.temporal_jitter  # ±15%亮度变化
        )

    def _on_epoch_end(self, prefix, batch, trainer, pl_module):
        """Log the current epoch's metrics and images.

        Parameters
        ----------
        prefix : str
            The split prefix (train, val, test).
        batch : dict
            A batch of data.
        trainer : pytorch_lightning.trainer.Trainer
            The trainer.
        pl_module : pytorch_lightning.LightningModule
            The model.
        """
        # lr, hr , hr_pan= batch["lr"], batch["hr"], batch["hr_pan"]
        if self.use_artificial_dataset:
            #  使用人造数据集
            lr_real, hr_real, hr_pan_real = batch["lr"], batch["hr"], batch["hr_pan"]

            lr = self.gener_new.generate(hr_real)  # DL_PAN
            # lr = self.generator.generate(hr_real)  #NIPS

            hr = hr_real
            hr_pan = hr_pan_real
        else:
            lr, hr, hr_pan = batch["lr"], batch["hr"], batch["hr_pan"]

        batch = self.move_batch_to_device(batch, pl_module.device)

        metrics = self.fetch_metrics_for_split(prefix, pl_module)

        loss_output, instance_psnr = self.get_losses_from_metrics(
            batch, metrics, pl_module, prefix
        )
        sr = self.get_output(loss_output)

        window_size, window_size_hr = self.calculate_window_size(lr, hr, pl_module)

        lr, hr, sr, hr_pan = self.extract_patches_from_images(
            sr, lr, hr, hr_pan, window_size, window_size_hr
        )

        instance_psnr = self.extract_psnr_values_for_patches(
            sr, lr, instance_psnr
        )

        lr = self.convert_lr_to_RGB_only_for_logging(lr)
        self.log_image_previews_to_wandb(
            sr,
            lr,
            hr,
            hr_pan,
            instance_psnr,
            pl_module,
            prefix,
            trainer,
        )

    def move_batch_to_device(self, batch, device):
        """Move all tensors from batch to the specified device.

        Parameters
        ----------
        batch : dict
            A batch of data.
        device : torch.device
            The device to move the tensors to.

        Returns
        -------
        dict
            The batch with all tensors moved to the specified device.
        """
        return {key: value.to(device) for key, value in batch.items()}

    def fetch_metrics_for_split(self, prefix, pl_module):
        """Fetch the metrics for the split.

        Parameters
        ----------
        prefix : str
            The prefix for the metrics.
        pl_module : pytorch_lightning.LightningModule
            The model.

        Returns
        -------
        dict, dict
            The metrics metrics for the current batch.
        """
        if prefix == "val":
            metrics = pl_module.val_metrics
        elif prefix == "train":
            metrics = pl_module.train_metrics
        elif prefix == "test":
            metrics = pl_module.test_metrics
        return metrics

    def get_losses_from_metrics(
            self, batch, metrics, pl_module, prefix
    ):
        """Get the losses that will be logged from the metrics.

        Parameters
        ----------

        batch : dict
            The batch.
        metrics : dict
            The metrics for the current batch.

        pl_module : pytorch_lightning.LightningModule
            The model.
        prefix : str
            The prefix for the metrics.

        Returns
        -------
        torch.Tensor, torch.Tensor, torch.Tensor
            The loss output, the PSNR values for the SR images in the batch.
        """
        loss_output = pl_module.loss(batch, metrics, prefix=prefix)
        instance_psnr = loss_output["metrics"][f"{prefix}/PSNR"].to("cpu")

        return loss_output, instance_psnr

    def get_output(self, loss_output):
        """Get the output images from the loss output.

        Parameters
        ----------
        loss_output : dict
            The loss output.

        Returns
        -------
        torch.Tensor, torch.Tensor
            The output images.
        """
        sr = loss_output["y_hat"].to("cpu")[:, None]
        return sr

    def calculate_window_size(self, lr, hr, pl_module):
        """Calculate the window size for the patches.

        Parameters
        ----------
        lr : torch.Tensor
            The LR image.
        hr : torch.Tensor
            The HR image.
        pl_module : pytorch_lightning.LightningModule
            The model.

        Returns
        -------
        tuple of int, tuple of int
            The window size for lr and hr.
        """
        _, _, _, lr_height, lr_width = lr.shape
        hr_height, hr_width = hr.shape[-2:]
        ratio_hr_lr_height, ratio_hr_lr_width = (
            hr_height / lr_height,
            hr_width / lr_width,
        )
        window_size = self.window_size or pl_module.hparams.input_size
        assert window_size <= tuple(pl_module.hparams.chip_size), "window_size too big"
        window_size_hr = (
            round(ratio_hr_lr_height * window_size[0]),
            round(ratio_hr_lr_width * window_size[0]),
        )

        return window_size, window_size_hr

    def extract_patches_from_images(
            self, sr, lr, hr, hr_pan, window_size, window_size_hr
    ):
        """Extract the patches from the images.

        Parameters
        ----------
        sr : torch.Tensor
            The SR image.
        hr : torch.Tensor
            The HR image.
        window_size : tuple of int
            The window size.
        window_size_hr : tuple of int
            The window size for the HR image.

        Returns
        -------
        torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor
            The extracted SR, LR, HR images.
        """
        lr = self.batch_patch_extractor(
            lr, window_size
        )  # (batch*patches, tensors/revisits, channels, patch_height, patch_width)
        hr = self.batch_patch_extractor(hr, window_size_hr)
        sr = self.batch_patch_extractor(sr, window_size_hr)
        hr_pan = self.batch_patch_extractor(hr_pan, window_size_hr)
        return lr, hr, sr, hr_pan

    @staticmethod
    def batch_patch_extractor(x, window_size):
        """Extract patches from a batch of images.

        Parameters
        ----------
        x : Tensor
            A batch of images (batch, tensors/revisits, channels, height, width).
        window_size : tuple of int
            The size of the patch to extract.

        Returns
        -------
        Tensor
            A batch of patches extracted from the images (batch*patches, tensors, channels, patch_height, patch_width), with patches extracted patches.
        """
        #    print(x.shape)
        batch, tensors, channels, height, width = x.shape
        x = extract_tensor_patches(
            x.view(batch, tensors * channels, height, width),
            window_size,
            stride=window_size,
        )
        _, patches, _, patch_height, patch_width = x.shape
        return x.view(
            batch, patches, tensors, channels, patch_height, patch_width
        ).view(batch * patches, tensors, channels, patch_height, patch_width)

    def extract_psnr_values_for_patches(
            self, sr, lr, instance_psnr
    ):
        """Extract the PSNR values for the patches.

        Parameters
        ----------
        sr : torch.Tensor
            The SR image.
        lr : torch.Tensor
            The LR image.
        instance_psnr : torch.Tensor
            The instance PSNR values.

        Returns
        -------
        torch.Tensor, torch.Tensor
            The extracted instance  PSNR values.
        """
        instance_psnr = (
            instance_psnr[None].expand(sr.shape[0] // lr.shape[0], -1).flatten()
        )

        return instance_psnr

    def convert_lr_to_RGB_only_for_logging(self, lr):
        """Convert the LR images to RGB only for logging.

        Parameters
        ----------
        lr : torch.Tensor
            The LR image.

        Returns
        -------
        torch.Tensor
            The LR image in RGB.
        """
        if (
                lr.shape[2] > 3
        ):  # If all bands are used, get only the RGB bands for WandB image logging
            lr = lr[:, :, S2_ALL_12BANDS["true_color_zero_index"]]
        return lr

    def log_image_previews_to_wandb(
            self,
            sr,
            lr,
            hr,
            hr_pan,
            instance_psnr,
            pl_module,
            prefix,
            trainer,
    ):
        """Log the current epoch's images to WandB.

        Parameters
        ----------
        sr : torch.Tensor
            The SR image.
        lr : torch.Tensor
            The LR image.
        hr : torch.Tensor
            The HR image.
        instance_psnr : torch.Tensor
            The instance PSNR values.
        pl_module : pl.LightningModule
            The model.
        prefix : str
            The split prefix (train, val, test).
        trainer : pytorch_lightning.trainer.Trainer
            The trainer.
        """
        number_of_rows_for_lr_revisits = int(sqrt(pl_module.hparams.revisits).round())
        # number_of_rows_for_lr_revisits = 4
        trainer.logger.experiment.log(
            {
                f"{prefix}-lr": [
                    wandb.Image(
                        make_grid(
                            lr_image,
                            nrow=number_of_rows_for_lr_revisits,
                            normalize=True,
                            scale_each=True,
                            padding=1,
                        ),
                        caption=f"lr",
                    )
                    for lr_image in lr
                ],
                f"{prefix}-hr": [
                    wandb.Image(
                        make_grid(hr_image, nrow=1, normalize=True, scale_each=True),
                        caption=f"hr",
                    )
                    for hr_image in hr
                ],
                f"{prefix}-hr_pan": [
                    wandb.Image(
                        make_grid(hr_pan_image, nrow=1, normalize=True, scale_each=True),
                        caption=f"hr",
                    )
                    for hr_pan_image in hr_pan
                ],
                f"{prefix}-sr": [
                    wandb.Image(
                        make_grid(image, nrow=1, normalize=True, scale_each=True),
                        caption=f"sr:{str(sr_psnr)}",
                    )
                    for image, sr_psnr in zip(sr, instance_psnr)
                ],
            }
        )

    def on_validation_epoch_end(self, trainer, pl_module):
        """Called at the end of the validation epoch.
        If the current epoch should be logged (i.e. the current epoch is a multiple of the log interval),
        then _on_epoch_end is called for the next batch of the validation set.

        Parameters
        ----------
        trainer : pytorch_lightning.trainer.Trainer
            The trainer.
        pl_module : pytorch_lightning.LightningModule
            The model.
        """
        if trainer.current_epoch % self.log_every_n_epochs == 0:
            batch = next(iter(self.val_dataloader))
            self._on_epoch_end("val", batch, trainer, pl_module)

    def on_test_epoch_end(self, trainer, pl_module):
        """Called at the end of the test epoch.

        Parameters
        ----------
        trainer : pytorch_lightning.trainer.Trainer
            The trainer.
        pl_module : pytorch_lightning.LightningModule
            The model.
        """
        max_batches_to_upload = 40  # 只上传前 20 个批次的数据
        for batch_idx, batch in enumerate(self.test_dataloader):
            if batch_idx >= max_batches_to_upload:
                break  # 只处理前 40 个批次
            self._on_epoch_end("test", batch, trainer, pl_module)

    def on_predict_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0):
        lr, hr, hr_pan = batch["lr"], batch["hr"], batch["hr_pan"]

        if self.use_artificial_dataset:
            lr = self.generator.generate(hr)
        batch = self.move_batch_to_device(batch, pl_module.device)
        ERGAS = ergas(outputs, hr.squeeze(1))
        outputs = outputs.to("cpu")[:, None]
        hr = hr.to("cpu")
        window_size, window_size_hr = self.calculate_window_size(lr, hr, pl_module)
        lr, hr, sr, hr_pan = self.extract_patches_from_images(
            outputs, lr, hr, hr_pan, window_size, window_size_hr
        )

        mse = F.mse_loss(sr, hr, reduction="none").mean(dim=(-1, -2, -3))  # over C, H, W
        psnr = -10.0 * torch.log10(mse)
        self.predict_log_image_previews_to_wandb(
            sr,
            lr,
            hr,
            hr_pan,
            psnr,
            pl_module,
            "predict",
            trainer,
        )

    def predict_log_image_previews_to_wandb(
            self,
            sr,
            lr,
            hr,
            hr_pan,
            psnr,
            pl_module,
            prefix,
            trainer,
    ):
        """Log the current epoch's images to WandB.

        Parameters
        ----------
        sr : torch.Tensor
            The SR image.
        lr : torch.Tensor
            The LR image.
        hr : torch.Tensor
            The HR image.
        instance_psnr : torch.Tensor
            The instance PSNR values.
        pl_module : pl.LightningModule
            The model.
        prefix : str
            The split prefix (train, val, test).
        trainer : pytorch_lightning.trainer.Trainer
            The trainer.
        """
        number_of_rows_for_lr_revisits = int(sqrt(pl_module.hparams.revisits).round())
        # number_of_rows_for_lr_revisits = 4
        trainer.logger.experiment.log(
            {
                f"{prefix}-lr": [
                    wandb.Image(
                        make_grid(
                            lr_image,
                            nrow=number_of_rows_for_lr_revisits,
                            normalize=True,
                            scale_each=True,
                            padding=1,
                        ),
                        caption=f"lr",
                    )
                    for lr_image in lr
                ],
                f"{prefix}-hr": [
                    wandb.Image(
                        make_grid(hr_image, nrow=1, normalize=True, scale_each=True),
                        caption=f"hr",
                    )
                    for hr_image in hr
                ],
                f"{prefix}-hr_pan": [
                    wandb.Image(
                        make_grid(hr_pan_image, nrow=1, normalize=True, scale_each=True),
                        caption=f"hr",
                    )
                    for hr_pan_image in hr_pan
                ],
                f"{prefix}-sr": [
                    wandb.Image(
                        make_grid(image, nrow=1, normalize=True, scale_each=True),
                        caption=f"sr:{str(sr_psnr)}",
                    )
                    for image, sr_psnr in zip(sr, psnr)
                ],
            }
        )
